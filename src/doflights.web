% $Id: d5141ba1b4cf22306cdecdacf77f77c6ce785193 $
\Title{doflights}

@* Subroutines for control of main program.

\ID{$Id: d5141ba1b4cf22306cdecdacf77f77c6ce785193 $}

@m FILE 'doflights.web'

@m MPI_messages 0
@m MPI_slave_messages 0

@m GPI_WEIGHTING 0  // Set to 1 for synthetic GPI weighting

@m send_data_no 0
@m send_data_meta_slave 1
@m send_data_all 2

@I macros.hweb
@I sysdep.hweb
@I string.hweb
@I sources.hweb
@I tally.hweb
@I output.hweb
@I detector.hweb
@I stat.hweb
@I netcdf.hweb
@I readfilenames.hweb
@I problem.hweb
@I random.hweb
@I mpi.hweb
@I background.hweb
@I snapshot_pdf.hweb
@I flight_frag.hweb

@ The unnamed module.

@a
      @<Functions and subroutines@>

@ The master controller.  Contains the outermost loop over source groups.
For each group, this routine breaks up the requested number of flights into
smaller chunks (of size |nfrag|) to be handled by the slave processors
(if any).

@<Functions...@>=
      subroutine do_flights_master(io_seed)

      define_dimen(chkpt_ind,num_chkpts)
      define_varp(chkpt_grp,INT,chkpt_ind)
      define_varp(chkpt_write,INT,chkpt_ind)
      define_varp(chkpt_nflights,INT,chkpt_ind)
      define_varp(estimator_factors,FLOAT,tally_ind)

      implicit_none_f77
      zn_common                                             // Common
      sp_common
      so_common
      pr_common
      de_common
      tl_common
      ou_common
      sa_common
      mp_common
      sn_common
      implicit_none_f90

      rn_seed_decl(io_seed)                            // Input / Output

      integer i,j,is,test,new_dim,pr_reac,jscore,      // Local
     $        iview,det,ibin,iauth,jscore_detail,
     $        jscore_total,icp,num_chkpts,group_tot_flights,
     $        nflights,nfrag,nparcel,num,nstart,nslaves,slave,
     $        send_data,post_arrays_allocd,isp,ip,
     $        last_sn_num_particles,have_snapshot_file
      integer index_parameters[tl_index_max],time[8]

      real mult,tot_scale_curr,prob,signal,wavelength,avg_part_per_sec
      real sig_det[30]
      logical need_scores
      character*30 date_str
      rn_seed_decl(seed)
      rn_seed_decl(temp_seed)
      rn_decl(temp_rand)
      rn_decls

      real extract_output_datum                   // External 
      external extract_output_datum

      @<Memory allocation interface@>
      st_decls
      mp_decls

      declare_varp(chkpt_grp)
      declare_varp(chkpt_write)
      declare_varp(chkpt_nflights)
      declare_varp(estimator_factors)

      var_alloc(estimator_factors)

      assert(ou_back_max >= pr_background_num) // Dimension for a local array

      if (so_restart == FALSE) then
         var_alloc(output_grp)
         var_alloc(output_weight_grp)
         var_alloc(output_num_flights)
         var_alloc(output_random_seed)

         post_arrays_allocd=FALSE      // Allocate |out_post_grp|, etc. later

         rn_seed_copy(io_seed,output_random_seed[1])
         
         if (so_spaced_seeds == TRUE && so_grps > 1) then
            do is=2,so_grps
               rn_seed_copy(output_random_seed[is-1],output_random_seed[is])
               call next_seed(so_seed_spacing,rn_seed_args(output_random_seed[is]))
            end do
         end if

         do is=1,so_grps
            output_weight_grp[is]=zero
            output_num_flights[is]=0
            do i=0,tally_size-1
               output_grp[is][i][o_mean]=zero
               output_grp[is][i][o_var]=zero
            end do
         end do

/*
   Need to set these parameters here since they are used in defining
   a local array in |fill_coupling_arrays|. Note: separate output variables
   are needed to permit use in a dimension statement.
*/
         output_index_1_min=zone_index_min[1]
         output_index_1_max=zone_index_max[1]
         output_index_2_min=zone_index_min[2]
         output_index_2_max=zone_index_max[2]

         sn_number_particles=0
         sn_particles_dim=mem_inc
         var_alloc(sn_particles_float)
         var_alloc(sn_particles_int)
      else if (so_restart == TRUE) then
         assert(output_old_file == TRUE) 
         post_arrays_allocd=TRUE
/*         
   The restart capability was never tested in time dependent mode. 
   Inserted the call to |nc_read_snapshot| here as a placeholder.
   We {\em do} need to allocate the snapshot arrays if there is
   no snapshot file.
*/
         call nc_read_snapshot(have_snapshot_file)
         if (have_snapshot_file == FALSE) then
            sn_number_particles=0
            sn_particles_dim=mem_inc
            var_alloc(sn_particles_float)
            var_alloc(sn_particles_int)
         end if
      else
         assert('Illegal value of so_restart' == ' ')
      end if

      if (so_sampling == so_direct) then
         call decimal_to_seed(so_seed_decimal,rn_seed_args(temp_seed))
         call next_seed(-1,rn_seed_args(temp_seed))
         rn_init(rn_seed_args(temp_seed),temp_rand)
         rn_next(so_direct_delta,temp_rand)
      else if (so_sampling == so_random) then
         so_direct_delta=zero
      else
         assert('Illegal value of so_sampling' == ' ')
      end if
@#if MPI && !MASTER_SAMPLE
      mpi_broadcast(so_direct_delta) 
@#endif
@#if 1
      stat_comp_flt=TRUE
      stat_comp_frag=TRUE // Set using |nfrag| and |nflights|?
@#else
      stat_comp_flt=FALSE
      stat_comp_frag=FALSE
@#endif
      stat_comp_fin=FALSE  // May be set differently on slaves

      call stat_init

      output_checkpoint=TRUE  // This flag will tell whether or not the
                              // output data have been post-processed.

      tot_scale_curr=zero
     
      num_chkpts=0
      var_realloca(chkpt_grp)
      var_realloca(chkpt_write)
      var_realloca(chkpt_nflights)
      do is=1,so_grps
         group_tot_flights=0
         if (so_nflights(is) > output_num_flights[is]) then
            do icp=1,max(so_chkpt(is),1)
               num_chkpts++
               var_realloca(chkpt_grp)
               var_realloca(chkpt_write)
               var_realloca(chkpt_nflights)
               chkpt_grp[num_chkpts]=is
               if (so_chkpt(is) == 0) then
                  chkpt_write[num_chkpts]=FALSE
               else
                  chkpt_write[num_chkpts]=TRUE
               end if
               if (icp < max(so_chkpt(is),1)) then
                  chkpt_nflights[num_chkpts]=
     $                 (so_nflights(is)-output_num_flights[is])
     $                     /max(so_chkpt(is),1)
                  group_tot_flights+=chkpt_nflights[num_chkpts]
               else
                  chkpt_nflights[num_chkpts]=so_nflights(is)-output_num_flights(is)-group_tot_flights
               end if
            end do
         else
/*
   This is for restarts with the same number of flights.
*/
            tot_scale_curr+=so_scale(is)*so_tot_curr(is)
         end if
      end do
      var_reallocb(chkpt_grp)
      var_reallocb(chkpt_write)
      var_reallocb(chkpt_nflights)

      is=0
      do icp=1,num_chkpts
         nflights=chkpt_nflights[icp]
         last_sn_num_particles=sn_number_particles

         if (is != chkpt_grp[icp]) then
            is=chkpt_grp[icp]
            rn_seed_copy(output_random_seed[is],seed)
            assert(so_spaced_seeds == FALSE || so_nflights(is) < so_seed_spacing)
            assert(stat_comp_fin == FALSE)
            do i=0,stat_wt_dim_fin-1      
               stat_wt_fin[i]=output_weight_grp[is]
            end do
         end if
@#if MPI
         nslaves=mpi_size-1
@#else
         nslaves=0
@#endif
         nfrag=max(nflights/max(100,10*nslaves),1)  // The size of each chunk

         nparcel=(nflights+nfrag-1)/nfrag // The number of chunks
@#if MPI
@#if MPI_messages
         write(stderr,*) 'nflights = ',nflights
@#endif
         do j=0,min(nparcel,nslaves)-1 // Send out the initial chunks
            nstart=j*nfrag
            num=min(nfrag,nflights-nstart)
            slave=j+1
@#if MPI_messages
            write(stderr,*) 'Send initial batch of ', num, ' to slave ',slave
            call date_time(time)
            call date_string(time,date_str)
            write(stderr,*) 'Time = ',date_str
@#endif
            call slave_send_flights(slave,output_num_flights[is]+nstart,num,so_args(is),seed)
            call next_seed(num,seed) // Advance random number seed
         end do
@#endif

         do j=min(nparcel,nslaves),nparcel-1 // Loop reading results and making additional requests
            nstart=j*nfrag
            num=min(nfrag,nflights-nstart)

                                             // Calculate the flights
@#if MPI
            if (nslaves>0) then
               call slave_receive_flag(slave)
@#if MPI_messages
               write(stderr,*) 'Slave ',slave,' is done with a batch'
               write(stderr,*) 'Sending ',num,' more to slave ',slave
               call date_time(time)
               call date_string(time,date_str)
               write(stderr,*) 'Time = ',date_str
@#endif
               send_data=send_data_no
               call slave_send_flag(slave,send_data)
               call slave_send_flights(slave,output_num_flights[is]+nstart,num,so_args(is),seed)
            else
               assert('Why is nslaves = 0 in an MPI run?' == ' ')
               call setup_flight_frag(output_num_flights[is]+nstart,num,so_args(is),seed)
               call do_flights(estimator_factors)
            end if
            
@#else
            call setup_flight_frag(output_num_flights[is]+nstart,num,so_args(is),seed) 
            call do_flights(estimator_factors)
/*
   Moved this inside the !MPI block since slaves send back data only at the
   end now. {\bf NOTE:} this will not work for the |nslaves| = 0 case.
*/
            @<Merge results@>
@#endif
            call next_seed(num,seed) // Advance the seed
         end do
         
@#if MPI
@#if MPI_messages
               write(stderr,*) 'Got to checkpoint before data send'
@#endif
         do j=0,min(nparcel,nslaves)-1 // Now just tell all slaves to send data.
            call slave_receive_flag(slave)
            if (nslaves < nparcel) then
               send_data=send_data_meta_slave
            else
               send_data=send_data_all  // Dispense with data aggregation
            end if
            call slave_send_flag(slave,send_data)
         end do	
/*
   The above will initiate the data compilation amongst all of the
   slaves.  All we need here is to loop over the final level
   of meta-slaves that send data here.  Note that this assumes
   that the |mpi_slave_map| for non-meta-slaves is {\em not} |mpi_degas2_root|.
*/
         do j=1,min(nparcel,nslaves)
            if (mpi_slave_map_m(mpi_nlevels,j) == mpi_degas2_root) then
               call slave_receive_flights(slave)
@#if MPI_messages
               write(stderr,*) 'Received batch back from slave ',slave
               call date_time(time)
               call date_string(time,date_str)
               write(stderr,*) 'Time = ',date_str
               call flush(stderr)
@#endif
               @<Merge results@>
            end if
         end do
@#endif
         if (sn_number_particles > last_sn_num_particles) then
            do ip=last_sn_num_particles+1,sn_number_particles
               sn_particles_float[ip][sn_float_pt_w]
     $              *=so_scale(is)*so_tot_curr(is)/output_weight_grp[is]
            end do
         end if
         if (stat_comp_frag == TRUE)   // |output_grp| here, too!
     $   call stat_wt_balance(output_weight_grp[is],stat_size_fin,stat_dim_fin,
     $           output_grp[is][0][o_mean],stat_wt_dim_fin,stat_wt_fin)
         if (stat_comp_fin == TRUE)
     $     call stat_decomp(stat_size_fin,stat_dim_fin,stat_fin,
     $                       stat_ps_dim_fin,stat_ptr2short_fin,
     $                       tally_size,output_grp[is][0][o_mean])

   // Replace |stat_wt_tot_fin| with |output_weight_grp|
         tot_scale_curr+=so_scale(is)*so_tot_curr(is)

         output_num_flights[is]+=nflights
         if (so_spaced_seeds == TRUE) then
            rn_seed_copy(seed,output_random_seed[is])
         else
/*
   Otherwise, when the run is complete, all the output seed for all
   groups will be that at the end of the run.
*/
            do isp=1,so_grps
               rn_seed_copy(seed,output_random_seed[isp])
            end do
         end if
/*
   Will write the same seed to the snapshot file in time dependent runs.
*/
         if (so_time_dependent == TRUE) call seed_to_decimal(seed,sn_seed_decimal)

@#if MPI_messages
         write(stderr,*) ' Before allocation of post arrays'
         write(stderr,*) ' post_arrays_allocd = ',post_arrays_allocd
         write(stderr,*) ' icp = ',icp
         write(stderr,*) ' chkpt_write[icp] = ',chkpt_write[icp]
         call flush(stderr)
@#endif
         if (post_arrays_allocd == FALSE 
     $        && (chkpt_write[icp] == TRUE || icp == num_chkpts)) then
            var_alloc(output_all)
@#if MPI_messages
         write(stderr,*) ' After allocation of output_all'
         call flush(stderr)
@#endif
            var_alloc(out_post_grp) 
@#if MPI_messages
         write(stderr,*) ' After allocation of out_post_grp'
         call flush(stderr)
@#endif
            var_alloc(out_post_all)
@#if MPI_messages
         write(stderr,*) ' After allocation of out_post_all'
         call flush(stderr)
@#endif
            var_alloc(output_2D_coupling)
@#if MPI_messages
         write(stderr,*) ' After allocation of output_2D_coupling'
         call flush(stderr)
@#endif
            post_arrays_allocd=TRUE
         end if
@#if MPI_messages
         write(stderr,*) ' After allocation of post arrays'
         call flush(stderr)
@#endif

         if (icp < num_chkpts && chkpt_write[icp] == TRUE) call nc_write_output
           
      end do           // Over |icp|
@#if !MPI
/*
   In the MPI case, these arrays are reallocated to the 
   exact size as they are being accumulated from the slaves.
*/
      if (sn_number_particles > 0) then
         sn_particles_dim=sn_number_particles
      else
         sn_particles_dim=1
      end if
      var_reallocb(sn_particles_float)
      var_reallocb(sn_particles_int)
@#endif
/*
   In preparation for combining results from various source groups, scale
   up according to total source rate for this group (presumably determined
   by physics) and the arbitrary scaling factor. Note that the latter does
   provide for real additional flexibility since the individual source
   currents are specified, making the total current derived. Hence, the 
   relative contribution to the sum over source groups can be more 
   efficiently altered via the scale factor. The expressions for 
   the total sample mean (|o_mean|) and variance of the sample
   mean (|o_var]|) are discussed in more detail in the Users Manual.

   In this sets of loops and the ones below them, the variance is
   effectively multiplied by $N/(N-1)$ to be consistent with textbook variance. 
*/
      do i=0,tally_size-1
         output_all[i][o_mean]=zero
         output_all[i][o_var]=zero
      end do

      do is=1,so_grps
         do i=0,tally_size-1
            prob=so_scale(is)*so_tot_curr(is)/tot_scale_curr
            out_post_grp[is][i][o_mean]=output_grp[is][i][o_mean]*prob
            output_all[i][o_mean]+=out_post_grp[is][i][o_mean]
            if (output_weight_grp[is] > zero) then
               out_post_grp[is][i][o_var]=output_grp[is][i][o_var]*prob**2/(output_weight_grp[is]-one)
            else
               assert(output_num_flights[is]==0 && output_grp[is][i][o_var]==zero)
               out_post_grp[is][i][o_var]=zero
            end if
            output_all[i][o_var]+=out_post_grp[is][i][o_var]
         end do
      end do
/*
   For the time dependent case, the input ``currents'' represent particles.
   Divide by the time interval to give the total (scaled) current
   units of particles per second, the same as in the time independent case.

   At this point, the |o_var| moments are converted to ``relative standard
   deviations'', i.e., the estimated error in the mean, $\sigma/\sqrt{N}$,
   normalized to the mean.  Note that the $1/N$ factor here cancels the
   $N$ in the $N/(N-1)$ above and, thus, does not appear.
*/
      if (so_time_dependent == TRUE) then
         avg_part_per_sec=tot_scale_curr/(so_time_final-so_time_initial)
      else
         avg_part_per_sec=tot_scale_curr
      end if
      do i=0,tally_size-1
         output_all[i][o_var]=sqrt(output_all[i][o_var])/max(abs(output_all[i][o_mean]),const(1.0,-100))
         output_all[i][o_mean]*=avg_part_per_sec
         do is=1,so_grps
            out_post_grp[is][i][o_var]=sqrt(out_post_grp[is][i][o_var])/max(abs(out_post_grp[is][i][o_mean]),const(1.0,-100))
            out_post_grp[is][i][o_mean]*=avg_part_per_sec
         end do
      end do
/*
   Post-process scores
*/
@#if MPI_messages
         write(stderr,*) ' Before post-processing of scores'
         call flush(stderr)
@#endif
      stat_comp_flt=FALSE               // To get diagonal pointers
      if (stat_dim_flt < tally_size) then
         var_realloc(stat_flt,stat_dim_flt-1,tally_size-1)
         stat_dim_flt=tally_size
      end if
@#if MPI_messages
      write(stderr,*)
     $   ' After allocation before post-processing of scores'
         call flush(stderr)
@#endif
      stat_size_flt=tally_size  // Had this above (wrongly so)

/*
   Since the post-processing routines eventually call |add_scores|, which
   now increments |stat_flt|, we must use the |stat_flt| array to hold
   the scores for the current source group. We could have chosen to
   to pass |stat_flt| to the post-processing arrays by common, but
   retained this interface to minimize changes. To do so, it must 
   be full-size!
*/
      need_scores=.false.
      do i=1,tally_type_num[tl_type_reaction]
         do pr_reac=1,pr_reaction_num+so_type_num
            jscore=tally_type_base[tl_type_reaction]+i-1
            if (tally_est_reaction[jscore][tl_est_post_process][pr_reac] 
     $                     > zero) then
               need_scores=.true.
            end if
         end do
      end do

      do is=1,so_grps 

         do i=0,tally_size-1
            stat_flt[i][o_mean]=out_post_grp[is][i][o_mean]
         end do
         assert(stat_size_flt==tally_size)
         call final_conversions(tl_cv_post,stat_flt)
            
         if (need_scores) then
            call post_process_test_scores(is,estimator_factors)
            call post_process_source_scores(is,estimator_factors)
         end if

         call final_conversions(tl_cv_output,stat_flt)

         do i=0,tally_size-1
            out_post_grp[is][i][o_mean]=stat_flt[i][o_mean]
// Now leave the |o_var| in here, now for lack of a better place
            if (is == 1) then
               out_post_all[i][o_mean]=stat_flt[i][o_mean]
// And put it here also for convenience
               out_post_all[i][o_var]=output_all[i][o_var]

            else
               out_post_all[i][o_mean]+=stat_flt[i][o_mean]
            end if
         end do
      end do       // is

@#if MPI
      do j=0,nslaves-1          // Kill off the slaves
         slave=j+1
         call slave_send_flights(slave,0,0,so_args(0),seed)
      end do
@#endif

      call fill_coupling_arrays

      do test=2,pr_test_num     // Exclude ``0'' test species

         jscore=string_lookup(trim(sp_sy(pr_test(test)))\/'alpha chord integrals',tally_name,tl_num)
         if (jscore > 0) then
            open(unit=diskout,file='chords_'\/trim(sp_sy(pr_test(test)))\/'_alpha.out',status='unknown')
            assert(tally_rank[jscore] == 1)
            do iview=1,tally_tab_index[jscore][1]
               index_parameters[tl_index_detector]=iview
               signal=extract_output_datum(index_parameters,1,
     $              out_post_all,o_mean,trim(sp_sy(pr_test(test)))\/'alpha chord integrals')
               write(diskout,'(2x,i3,6x,1pe13.4)') iview,signal
            end do
            close(unit=diskout)
         end if

         jscore_detail=string_lookup(trim(sp_sy(pr_test(test)))\/'alpha spectrum detail',tally_name,tl_num)
         jscore_total=string_lookup(trim(sp_sy(pr_test(test)))\/'alpha spectrum',tally_name,tl_num)
         if (jscore_detail > 0 && jscore_total > 0) then
            open(unit=diskout,file='spectrum_'\/trim(sp_sy(pr_test(test)))\/'_alpha.out',status='unknown')
            assert(tally_indep_var[jscore_detail][3] == tl_index_wavelength_bin)
            assert(tally_indep_var[jscore_detail][2] == tl_index_test_author)
            assert(tally_tab_index[jscore_detail][1] >= 1) // views
            det=de_lookup('Halpha spectrum')   // Same name for all isotopes
            assert(detector_spacing[det] == de_spacing_linear)
            assert(detector_tab_index[det] == tally_tab_index[jscore_detail][3])
            assert(tally_tab_index[jscore_detail][2] <= 30)   // |sig_det| dimension
            assert(tally_indep_var[jscore_total][2] == tl_index_wavelength_bin)
            assert(tally_tab_index[jscore_total][1] >= 1) // views
            assert(detector_tab_index[det] == tally_tab_index[jscore_total][2])
            do iview=1,tally_tab_index[jscore_detail][1]
               index_parameters[tl_index_detector]=iview
               write(diskout,*) 'Spectrum view number ',iview
               do ibin=1,tally_tab_index[jscore_detail][3]
                  index_parameters[tl_index_wavelength_bin]=ibin
                  do iauth=1,tally_tab_index[jscore_detail][2]
                     index_parameters[tl_index_test_author]=iauth
                     signal=extract_output_datum(index_parameters,1,
     $                    out_post_all,o_mean,trim(sp_sy(pr_test(test)))\/'alpha spectrum detail')
                     sig_det[iauth]=signal
                  end do
                  signal=extract_output_datum(index_parameters,1,
     $                 out_post_all,o_mean,trim(sp_sy(pr_test(test)))\/'alpha spectrum')
                  wavelength=detector_min[det]+(areal(ibin)-0.5)*detector_delta[det]
                  write(diskout,'(1pe16.6,22e13.4)') wavelength,signal,(sig_det[iauth],iauth=1,tally_tab_index[jscore_detail][2])
               end do
               write(diskout,*) '-----------------------------------------------------'
            end do

            close(unit=diskout)

         end if
      end do

      output_checkpoint=FALSE
      call nc_write_output
@#if 0
/*
   Moved up a level
*/
      call clear_output
@#endif
      rn_seed_copy(output_random_seed[1],io_seed)

      call clear_stat

      var_free(chkpt_grp)
      var_free(chkpt_write)
      var_free(chkpt_nflights)
      var_free(estimator_factors)

      return
      end

@ Merge results from a fragment.  This is the original version of this
bit of code and still the one used by the master (and without MPI).

@<Merge results@>=

            if (stat_comp_frag == TRUE)  // Moved from |do_flights|
     $      call stat_wt_balance(stat_wt_tot_frag,stat_size_frag,stat_dim_frag,
     $        stat_frag,stat_wt_dim_frag,stat_wt_frag)
            if (min(stat_size_fin+stat_size_frag,tally_size)>stat_dim_fin) then
               assert(stat_comp_fin == TRUE) 
               mult=max(one,areal(stat_size_frag)/areal(stat_mem_inc_fin))
               new_dim=min(stat_dim_fin+int(mult*stat_mem_inc_fin),tally_size)
               var_realloc(stat_fin,stat_dim_fin-1,new_dim-1)
               var_realloc(stat_ptr2full_fin,stat_pf_dim_fin-1,new_dim-1)
               var_realloc(stat_wt_fin,stat_wt_dim_fin-1,new_dim-1)
               stat_dim_fin=new_dim
               stat_pf_dim_fin=new_dim
               stat_wt_dim_fin=new_dim
            end if
                                  // Accumulating directly into |output_grp|! 
            assert(stat_comp_fin == FALSE && stat_dim_fin == tally_size)
/*
   These are temporary diagnostic statements.
*/
            if (stat_wt_tot_frag < zero) then
               write(stderr,*) ' Bad value of stat_wt_tot_frag = ',stat_wt_tot_frag
               write(stderr,*) ' slave = ',slave
               assert(stat_wt_tot_frag >= zero)
               call flush(stderr)
            end if
/*
   End of temporary diagnostics.
*/               
            call stat_acc(stat_comp_frag,stat_wt_tot_frag,stat_size_frag,
     $           stat_dim_frag,stat_frag,stat_pf_dim_frag,stat_ptr2full_frag,
     $           stat_ps_dim_frag,stat_ptr2short_frag,
     $           stat_comp_fin,output_weight_grp[is],stat_size_fin,
     $           stat_dim_fin,output_grp[is][0][o_mean],
     $           stat_pf_dim_fin,stat_ptr2full_fin,
     $           stat_ps_dim_fin,stat_ptr2short_fin,stat_wt_dim_fin,
     $           stat_wt_fin)

@ Merge results from a fragment into |stat_fin|.  This is the same as the above
passage, but with the merger done into |stat_fin|, with total weight going into
|stat_wt_tot_fin|.  This is used only on meta-slaves during their data collection.

@<Merge fin results@>=

            if (stat_comp_frag == TRUE)
     $      call stat_wt_balance(stat_wt_tot_frag,stat_size_frag,stat_dim_frag,
     $        stat_frag,stat_wt_dim_frag,stat_wt_frag)
            if (min(stat_size_fin+stat_size_frag,tally_size)>stat_dim_fin) then
               assert(stat_comp_fin == TRUE) 
               mult=max(one,areal(stat_size_frag)/areal(stat_mem_inc_fin))
               new_dim=min(stat_dim_fin+int(mult*stat_mem_inc_fin),tally_size)
               var_realloc(stat_fin,stat_dim_fin-1,new_dim-1)
               var_realloc(stat_ptr2full_fin,stat_pf_dim_fin-1,new_dim-1)
               var_realloc(stat_wt_fin,stat_wt_dim_fin-1,new_dim-1)
               stat_dim_fin=new_dim
               stat_pf_dim_fin=new_dim
               stat_wt_dim_fin=new_dim
            end if
/*
   These are temporary diagnostic statements.
*/
            if (stat_wt_tot_frag < zero) then
               write(stderr,*) ' Bad value of stat_wt_tot_frag = ',stat_wt_tot_frag
               write(stderr,*) ' slave = ',mpi_rank
               assert(stat_wt_tot_frag >= zero)
               call flush(stderr)
            end if
/*
   End of temporary diagnostics.
*/               
            call stat_acc(stat_comp_frag,stat_wt_tot_frag,stat_size_frag,
     $           stat_dim_frag,stat_frag,stat_pf_dim_frag,stat_ptr2full_frag,
     $           stat_ps_dim_frag,stat_ptr2short_frag,
     $           stat_comp_fin,stat_wt_tot_fin,stat_size_fin,
     $           stat_dim_fin,stat_fin,
     $           stat_pf_dim_fin,stat_ptr2full_fin,
     $           stat_ps_dim_fin,stat_ptr2short_fin,stat_wt_dim_fin,
     $           stat_wt_fin)

@ Send frag.  I.e., send a |stat_frag| object.  Defining this not because
it gets used multiple times, but to improve readability.  However, the
subsequent chunk of code is intended to be exactly the same thing, 
but for |stat_fin|.

@<Send frag@>=
      
      tag=400

@#if MPI_slave_messages
      write(diskout+3,*) ' Before final data send calls; tag = ',tag
      call flush(diskout+3)
@#endif
      call MPI_send(stat_comp_frag,1,mpi_int,mpi_slave_map_m(ilevel,mpi_rank),tag,comm_world_dup,mpi_err)
      tag++
      call MPI_send(stat_wt_tot_frag,1,mpi_real,mpi_slave_map_m(ilevel,mpi_rank),tag,comm_world_dup,mpi_err)
      tag++
      call MPI_send(stat_size_frag,1,mpi_int,mpi_slave_map_m(ilevel,mpi_rank),tag,comm_world_dup,mpi_err)

@#if MPI_slave_messages
      write(diskout+3,*) ' During final data send calls (1a); tag = ',tag
      call flush(diskout+3)
@#endif
      tag++
      call MPI_send(stat_dim_frag,1,mpi_int,mpi_slave_map_m(ilevel,mpi_rank),tag,comm_world_dup,mpi_err)
      tag++
@#if MPI_slave_messages
      write(diskout+3,*) ' Before sending stat_frag array. Size = ',array_size(stat_frag),'; dest = ',mpi_slave_map_m(ilevel,mpi_rank),'; tag = ',tag,'; mpi_err = ',mpi_err 
      call flush(diskout+3)
@#endif
      call MPI_send(stat_frag,array_size(stat_frag),mpi_real,mpi_slave_map_m(ilevel,mpi_rank),tag,comm_world_dup,mpi_err)
@#if MPI_slave_messages
      write(diskout+3,*) ' After sending stat_frag array...'
      call flush(diskout+3)
@#endif

      tag++
      call MPI_send(stat_pf_dim_frag,1,mpi_int,mpi_slave_map_m(ilevel,mpi_rank),tag,comm_world_dup,mpi_err)
      tag++
      call MPI_send(stat_ptr2full_frag,array_size(stat_ptr2full_frag),mpi_int,mpi_slave_map_m(ilevel,mpi_rank),tag,comm_world_dup,mpi_err)

@#if MPI_slave_messages
      write(diskout+3,*) ' During final data send calls (2a); tag = ',tag
      call flush(diskout+3)
@#endif
      tag++
      call MPI_send(stat_ps_dim_frag,1,mpi_int,mpi_slave_map_m(ilevel,mpi_rank),tag,comm_world_dup,mpi_err)
      tag++
      call MPI_send(stat_ptr2short_frag,array_size(stat_ptr2short_frag),mpi_int,mpi_slave_map_m(ilevel,mpi_rank),tag,comm_world_dup,mpi_err)

      tag++
      call MPI_send(stat_wt_dim_frag,1,mpi_int,mpi_slave_map_m(ilevel,mpi_rank),tag,comm_world_dup,mpi_err)
      tag++
      call MPI_send(stat_wt_frag,array_size(stat_wt_frag),mpi_real,mpi_slave_map_m(ilevel,mpi_rank),tag,comm_world_dup,mpi_err)
@#if MPI_slave_messages
      write(diskout+3,*) ' After final data send calls; tag = ',tag
      call flush(diskout+3)
@#endif
      tag++
      call MPI_send(sn_number_particles,1,mpi_int,mpi_slave_map_m(ilevel,mpi_rank),tag,comm_world_dup,mpi_err)

      if (sn_number_particles > 0) then
         tag++
         call MPI_send(sn_particles_float,array_unit(sn_particles_float)*sn_number_particles,mpi_real,mpi_slave_map_m(ilevel,mpi_rank),tag,comm_world_dup,mpi_err)
         
         tag++
         call MPI_send(sn_particles_int,array_unit(sn_particles_int)*sn_number_particles,mpi_int,mpi_slave_map_m(ilevel,mpi_rank),tag,comm_world_dup,mpi_err)
      end if
/*
   Reinitialize the arrays just sent.  This is otherwise done within |stat_acc|, but
   these arrays are locally only the ``base'' in |stat_acc|.
*/
      call stat_zero(stat_wt_tot_frag,stat_comp_frag,stat_size_frag,stat_ps_dim_frag,
     $     stat_ptr2short_frag,stat_dim_frag,stat_frag)

@ Send fin.  I.e., send a |stat_fin| object.  This is the exact same bit of code as
the |Send frag| section above, but with |_frag| replaced by |_fin|.

@<Send fin@>=
      
      tag=400

@#if MPI_slave_messages
      write(diskout+3,*) ' Before final data send calls; tag = ',tag
      call flush(diskout+3)
@#endif
      call MPI_send(stat_comp_fin,1,mpi_int,mpi_slave_map_m(ilevel,mpi_rank),tag,comm_world_dup,mpi_err)
      tag++
      call MPI_send(stat_wt_tot_fin,1,mpi_real,mpi_slave_map_m(ilevel,mpi_rank),tag,comm_world_dup,mpi_err)
      tag++
      call MPI_send(stat_size_fin,1,mpi_int,mpi_slave_map_m(ilevel,mpi_rank),tag,comm_world_dup,mpi_err)

@#if MPI_slave_messages
      write(diskout+3,*) ' During final data send calls (1); tag = ',tag
      call flush(diskout+3)
@#endif
      tag++
      call MPI_send(stat_dim_fin,1,mpi_int,mpi_slave_map_m(ilevel,mpi_rank),tag,comm_world_dup,mpi_err)
      tag++
      call MPI_send(stat_fin,array_size(stat_fin),mpi_real,mpi_slave_map_m(ilevel,mpi_rank),tag,comm_world_dup,mpi_err)

      tag++
      call MPI_send(stat_pf_dim_fin,1,mpi_int,mpi_slave_map_m(ilevel,mpi_rank),tag,comm_world_dup,mpi_err)
      tag++
      call MPI_send(stat_ptr2full_fin,array_size(stat_ptr2full_fin),mpi_int,mpi_slave_map_m(ilevel,mpi_rank),tag,comm_world_dup,mpi_err)

@#if MPI_slave_messages
      write(diskout+3,*) ' During final data send calls (2); tag = ',tag
      call flush(diskout+3)
@#endif
      tag++
      call MPI_send(stat_ps_dim_fin,1,mpi_int,mpi_slave_map_m(ilevel,mpi_rank),tag,comm_world_dup,mpi_err)
      tag++
      call MPI_send(stat_ptr2short_fin,array_size(stat_ptr2short_fin),mpi_int,mpi_slave_map_m(ilevel,mpi_rank),tag,comm_world_dup,mpi_err)

      tag++
      call MPI_send(stat_wt_dim_fin,1,mpi_int,mpi_slave_map_m(ilevel,mpi_rank),tag,comm_world_dup,mpi_err)
      tag++
      call MPI_send(stat_wt_fin,array_size(stat_wt_fin),mpi_real,mpi_slave_map_m(ilevel,mpi_rank),tag,comm_world_dup,mpi_err)
@#if MPI_slave_messages
      write(diskout+3,*) ' After final data send calls; tag = ',tag
      call flush(diskout+3)
@#endif
      tag++
      call MPI_send(sn_number_particles,1,mpi_int,mpi_slave_map_m(ilevel,mpi_rank),tag,comm_world_dup,mpi_err)

      if (sn_number_particles > 0) then
         tag++
         call MPI_send(sn_particles_float,array_unit(sn_particles_float)*sn_number_particles,mpi_real,mpi_slave_map_m(ilevel,mpi_rank),tag,comm_world_dup,mpi_err)
         
         tag++
         call MPI_send(sn_particles_int,array_unit(sn_particles_int)*sn_number_particles,mpi_int,mpi_slave_map_m(ilevel,mpi_rank),tag,comm_world_dup,mpi_err)
      end if
/*
   Reinitialize the arrays just sent.  This is otherwise done within |stat_acc|, but
   these arrays are locally only the ``base'' in |stat_acc|.
*/
      call stat_zero(stat_wt_tot_fin,stat_comp_fin,stat_size_fin,stat_ps_dim_fin,
     $  stat_ptr2short_fin,stat_dim_fin,stat_fin)

@ Prepare a set of flight fragment data.  This version called in both
parallel and serial cases.

@#if GPI_WEIGHTING
@m nbins 16
@m ntest 3
@#endif

@<Functions...@>=
      subroutine setup_flight_frag(nstart,nflights,so_dummy(isource),seed0)
      implicit_none_f77
      ff_common
      tl_common
      pr_common
      so_common
      implicit_none_f90

      integer nstart,nflights        // Input
      so_decl(isource)
      rn_seed_decl(seed0)

      integer j,ipart,kseg,xseg      // Local
      rn_seed_decl(seed)
      rn_decl(rand)
      pt_decl(x)
      logical init
      save init
      data init/.true./
@#if GPI_WEIGHTING
/*
   For checking sampling with relative weighting
*/
      integer ibin,itest
      integer binned_number[nbins][ntest],total_number[ntest]
      real rmin,rmax,rmaj,avg_weight,total_weight
      real binned_weight[nbins],binned_test_weight[nbins][ntest],
     $     total_test_weight[ntest]
      data rmin/dconst(0.82)/,rmax/dconst(0.98)/
      save binned_number,total_number,rmin,rmax,total_weight,
     $     binned_weight,binned_test_weight,total_test_weight
@#endif
      ff_decls
      @<Memory allocation interface@>

@#if ((MPI && MASTER_SAMPLE) || !MPI)
      if (init) then
         ff_particles_dim=nflights
         var_alloc(ff_particles_int)
         var_alloc(ff_particles_float)
         var_alloc(ff_ran_index)
         var_alloc(ff_ran_array)
@#if GPI_WEIGHTING
/*
   For sampling test
*/
         do ibin=1,nbins
            binned_weight[ibin]=zero
            do itest=1,ntest
               binned_number[ibin][itest]=0
               binned_test_weight[ibin][itest]=zero
            end do
         end do
         total_weight=zero	
         do itest=1,ntest
            total_number[itest]=0
            total_test_weight[itest]=zero
         end do
@#endif
         init=.false.
      else if (nflights > ff_particles_dim) then
         var_realloc(ff_particles_int,ff_particles_dim,nflights)
         var_realloc(ff_particles_float,ff_particles_dim,nflights)
         var_realloc(ff_ran_index,ff_particles_dim,nflights)
         var_realloc(ff_ran_array,ff_particles_dim,nflights)
         ff_particles_dim=nflights
      end if
      ff_number_particles=nflights
@#endif
/*
   Use a local |seed| and advance one flight at a time,
   as was done previously in subroutine |do_flights|.
*/
      rn_seed_copy(seed0,seed)

      do j=nstart,nstart+nflights-1
         ipart=j-nstart+1
         rn_init(seed,rand)
         call sample_sources(tl_est_collision,isource,j,
     $                       pt_args(x),rn_args(rand),kseg)
/*
   This macro copies the resulting particle and random objects
   into the |flight_frag| array.  These are then communicated
   to other routines via common blocks and MPI calls.
*/
         xseg=source_segment_ptr[so_base(isource)+kseg]
         ff_copy(ipart,j,isource,kseg,xseg,so_type(isource),
     $           so_root_sp(isource),x,rand)
@#if GPI_WEIGHTING
         if (isource == 2) then
            rmaj=sqrt(lc_x(pt_loc(x))[1]**2+lc_x(pt_loc(x))[2]**2)
            assert((rmaj > rmin) && (rmaj < rmax))
            ibin=int((rmaj-rmin)*const(1.,2))+1
            assert((ibin >=1) && (ibin <= nbins))
            binned_weight[ibin]+=pt_w(x)
            total_weight+=pt_w(x)
            itest=pt_test(x)-1
            binned_test_weight[ibin][itest]+=pt_w(x)
            total_test_weight[itest]+=pt_w(x)
            binned_number[ibin][itest]++
            total_number[itest]++
         end if
@#endif
         call next_seed(1,seed)
      end do
@#if GPI_WEIGHTING
      if ((isource == 2) 
     $    && (nstart+nflights == so_nflights(isource))) then
         avg_weight=total_weight/areal(nbins)      
         do ibin=1,nbins
            binned_weight[ibin]/=avg_weight
         end do
         open(unit=diskout,file='snapshot_samples',status='unknown')
         write(diskout,*) ' Total D = ',total_number[1],
     $        ' Total D2 = ', total_number[2],
     $        ' Total D2+ = ',total_number[3]
         write(diskout,'(a,1p,e12.5)') ' Total weight = ',total_weight
         write(diskout,'(1p,3(a,e12.5,2x))') ' D:  ',total_test_weight[1],
     $        ' D2:  ',total_test_weight[2],
     $        ' D2+:  ',total_test_weight[3]
         write(diskout,'(a)') ' bin  R(m)        D         D2         D2+     RelWt           D_wt          D2_wt         D2p_wt'
         do ibin=1,nbins
            write(diskout,'(i3,2x,f6.2,2x,3(i9,2x),4(1p,e12.5,2x))') ibin,
     $           rmin+areal(ibin)*const(1.,-2),
     $           (binned_number[ibin][itest],itest=1,ntest),
     $           binned_weight[ibin],
     $           (binned_test_weight[ibin][itest],itest=1,ntest)
         end do
         close(unit=diskout)

      end if
@#endif
      return
      end

@ Send data to slave.

@<Functions...@>=
@#if MPI
      subroutine slave_send_flights(slave,nstart,nflights,so_dummy(isource),seed)
      implicit_none_f77
      mp_common
      sa_common
      ff_common
      implicit_none_f90

      integer slave,nstart,nflights      // Input
      so_decl(isource)
      rn_seed_decl(seed)

      integer tag                        // Local
      integer time[8]
      character*30 date_str

      mp_decls
      @<Memory allocation interface@>

      if (nflights == 0) then
         tag=100
         call MPI_send(nflights,1,mpi_int,slave,tag,comm_world_dup,mpi_err)
         return
      end if
@#if MASTER_SAMPLE
/*
   Sample the source particles and pack them into the 
   |flight_frag| object, conveyed here by |ff_common|.
*/
      call setup_flight_frag(nstart,nflights,so_args(isource),seed)
@#if MPI_messages
      write(stderr,*) 'After setup_flight_frag for slave ',slave
      call date_time(time)
      call date_string(time,date_str)
      write(stderr,*) 'Time = ',date_str
@#endif
/*
  MPI_send ff objects to slave.
*/
      tag=100
      call MPI_send(ff_number_particles,1,mpi_int,slave,tag,comm_world_dup,mpi_err)
      tag++
      call MPI_send(ff_particles_int,array_size(ff_particles_int),mpi_int,
     $              slave,tag,comm_world_dup,mpi_err)
      tag++
      call MPI_send(ff_particles_float,array_size(ff_particles_float),mpi_real,
     $              slave,tag,comm_world_dup,mpi_err)
      tag++
      call MPI_send(ff_ran_index,array_size(ff_ran_index),mpi_int,slave,
     $              tag,comm_world_dup,mpi_err)
      tag++
      call MPI_send(ff_ran_array,array_size(ff_ran_array),mpi_real,
     $              slave,tag,comm_world_dup,mpi_err)
@#else
      tag=100
      call MPI_send(nflights,1,mpi_int,slave,tag,comm_world_dup,mpi_err)
      tag++
      call MPI_send(nstart,1,mpi_int,slave,tag,comm_world_dup,mpi_err)
      tag++
      call MPI_send(isource,1,mpi_int,slave,tag,comm_world_dup,mpi_err)
      tag++
      call MPI_send(seed,ran_s,mpi_int,slave,tag,comm_world_dup,mpi_err)
@#endif
      
      return
      end
@#endif

@ Receive notice from a slave indicating that it is done with current batch.

@<Functions...@>=
@#if MPI
      subroutine slave_receive_flag(slave)
      implicit_none_f77
      mp_common
      implicit_none_f90
      mp_decls
      integer slave
      integer tag,done_flag
      @<Memory allocation interface@>

      tag=200
      call MPI_recv(done_flag,1,mpi_int,MPI_ANY_SOURCE,tag,comm_world_dup,mpi_status,mpi_err)
      slave=mpi_status(MPI_SOURCE)
      assert(done_flag == TRUE)
      return
      end
@#endif

@ Send a flag to a slave instructing it to send back data from its flights.

@<Functions...@>=
@#if MPI
      subroutine slave_send_flag(slave,send_data)
      implicit_none_f77
      mp_common
      implicit_none_f90
      mp_decls
      integer slave,send_data         // Input
      integer tag                     // Local
      @<Memory allocation interface@>

      tag=300
      call MPI_send(send_data,1,mpi_int,slave,tag,comm_world_dup,mpi_err)

      return
      end
@#endif

@ Receive data from flights run by a slave.

@<Functions...@>=
@#if MPI
      subroutine slave_receive_flights(slave)
      implicit_none_f77
      mp_common
      sa_common
      sn_common
      implicit_none_f90
      mp_decls
      integer slave,nstart,nflights
      integer tag,new_dim,new_sn_num_particles
      @<Memory allocation interface@>

      tag=400
      call MPI_recv(stat_comp_frag,1,mpi_int,MPI_ANY_SOURCE,tag,comm_world_dup,mpi_status,mpi_err)
      slave=mpi_status(MPI_SOURCE);
@#if MPI_slave_messages
      write(diskout+3,*) ' Recieving data from rank ', slave
      call flush(diskout+3)
@#endif
      tag++
      call MPI_recv(stat_wt_tot_frag,1,mpi_real,slave,tag,comm_world_dup,mpi_status,mpi_err)
      tag++
      call MPI_recv(stat_size_frag,1,mpi_int,slave,tag,comm_world_dup,mpi_status,mpi_err)

      tag++
      call MPI_recv(new_dim,1,mpi_int,slave,tag,comm_world_dup,mpi_status,mpi_err)
      if (new_dim>stat_dim_frag) then
         var_realloc(stat_frag,stat_dim_frag-1,new_dim-1)
         stat_dim_frag=new_dim
      end if
      tag++
@#if MPI_slave_messages
      write(diskout+3,*) ' Before recieving stat_frag array. Size = ',array_size(stat_frag),'; origin = ',slave,'; tag = ',tag,'; mpi_status = ',mpi_status,'; mpi_err = ',mpi_err 
      call flush(diskout+3)
@#endif
      call MPI_recv(stat_frag,array_size(stat_frag),mpi_real,slave,tag,comm_world_dup,mpi_status,mpi_err)
@#if MPI_slave_messages
      write(diskout+3,*) ' After recieving stat_frag array.'
      call flush(diskout+3)
@#endif

      tag++
      call MPI_recv(new_dim,1,mpi_int,slave,tag,comm_world_dup,mpi_status,mpi_err)
      if (new_dim>stat_pf_dim_frag) then
         var_realloc(stat_ptr2full_frag,stat_pf_dim_frag-1,new_dim-1)
         stat_pf_dim_frag=new_dim
      end if
      tag++
      call MPI_recv(stat_ptr2full_frag,array_size(stat_ptr2full_frag),mpi_int,slave,tag,comm_world_dup,mpi_status,mpi_err)

      tag++
      call MPI_recv(new_dim,1,mpi_int,slave,tag,comm_world_dup,mpi_status,mpi_err)
      if (new_dim>stat_ps_dim_frag) then
         var_realloc(stat_ptr2short_frag,stat_ps_dim_frag-1,new_dim-1)
         stat_ps_dim_frag=new_dim
      end if
      tag++
      call MPI_recv(stat_ptr2short_frag,array_size(stat_ptr2short_frag),mpi_int,slave,tag,comm_world_dup,mpi_status,mpi_err)

      tag++
      call MPI_recv(new_dim,1,mpi_int,slave,tag,comm_world_dup,mpi_status,mpi_err)
      if (new_dim>stat_wt_dim_frag) then
         var_realloc(stat_wt_frag,stat_wt_dim_frag-1,new_dim-1)
         stat_wt_dim_frag=new_dim
      end if

      tag++
      call MPI_recv(stat_wt_frag,array_size(stat_wt_frag),mpi_real,slave,tag,comm_world_dup,mpi_status,mpi_err)

      tag++
      call MPI_recv(new_sn_num_particles,1,mpi_int,slave,tag,comm_world_dup,mpi_status,mpi_err)

@#if MPI_slave_messages
      write(diskout+3,*) ' After data recieve'
      call flush(diskout+3)
@#endif
      if (new_sn_num_particles > 0) then
/*
   The reallocations of these arrays roughly parallel that of some in
   |definegeometry2d| (e.g., |wall_nodes|) with initial reallocations 
   done in |mem_inc| steps via |var_realloca| (in    |build_snapshot_pdf|) 
   and then here in larger steps.  |var_reallocc| works better than just
   |var_realloc| in that it expects the array dimension to be the
   appropriate multiple of |mem_inc|.

   One problem is that the actual array dimension is not the second
   argument to |var_reallocc|; we instead invoke the |last_uboundc| macro 
   to get it.  By setting |sn_particles_dim| thus, the |var_free| calls 
   for these arrays will finish without complaint.  Note that in general the
   tactic should be to call |var_reallocb| once the array finishes 
   growing.  That point is difficult to determine for these arrays.
*/
         if (sn_number_particles+new_sn_num_particles > sn_particles_dim) then
            var_reallocc(sn_particles_float,sn_particles_dim,sn_number_particles+new_sn_num_particles)
            var_reallocc(sn_particles_int,sn_particles_dim,sn_number_particles+new_sn_num_particles)
            sn_particles_dim=last_uboundc(sn_particles_float,sn_number_particles+new_sn_num_particles)
         end if
         
         tag++
         call MPI_recv(sn_particles_float[sn_number_particles+1][1],
     $                 array_unit(sn_particles_float)*new_sn_num_particles,
     $                 mpi_real,slave,tag,comm_world_dup,mpi_status,mpi_err)

         tag++
         call  MPI_recv(sn_particles_int[sn_number_particles+1][1],
     $                 array_unit(sn_particles_int)*new_sn_num_particles,
     $                 mpi_int,slave,tag,comm_world_dup,mpi_status,mpi_err)

         sn_number_particles+=new_sn_num_particles
      end if

break:  continue
      return
      end
@#endif

@ The slave routine.  This loops waiting for work and sending the
results back to the master.  It is only defined for MPI.

@<Functions...@>=
@#if MPI
      subroutine do_flights_slave
      implicit_none_f77
      mp_common
      sa_common
      ff_common
      pr_common
      so_common
      tl_common
      sn_common
      implicit_none_f90
      mp_decls
      integer nstart,nflights,tag,done_flag,send_data,
     $        i,j,ilevel,min_level,islave,sending_slave,
     $        new_dim
      integer time[8]
      real mult
      character*30 date_str
      logical init
      save init
      data init/.true./

@#if MPI_slave_messages
      character*6 cslave
@#endif
      so_decl(isource)
      rn_seed_decl(seed)
      @<Memory allocation interface@>
      tl_decls

      declare_varp(estimator_factors)
      var_alloc(estimator_factors)

      sn_number_particles=0
      sn_particles_dim=mem_inc
      var_alloc(sn_particles_float)
      var_alloc(sn_particles_int)
@#if !MASTER_SAMPLE
      mpi_broadcast(so_direct_delta)
@#endif

@#if MPI_slave_messages
      assert((mpi_rank > 0) && (mpi_rank < 1000000))
      write(cslave,'(i6.6)') mpi_rank
      open(unit=diskout+3,
     $     file='dps_slave_' \/ cslave,status='unknown')
@#endif

      stat_comp_flt=TRUE
      stat_comp_frag=TRUE // Set using |nfrag| and |nflights|?
      stat_comp_fin=TRUE  // Using the |stat_fin| arrays for intermediate aggregation levels

      call stat_init

loop: continue
      tag=100
@#if MPI_slave_messages
      write(diskout+3,*) ' Beginning receive calls; tag = ',tag
      call date_time(time)
      call date_string(time,date_str)
      write(diskout+3,*) 'Time = ',date_str
@#endif
      call MPI_recv(ff_number_particles,1,mpi_int,mpi_degas2_root,tag,
     $              comm_world_dup,mpi_status,mpi_err)
      if (ff_number_particles == 0) goto break
      tag++
      if (init) then
         ff_particles_dim=ff_number_particles
         var_alloc(ff_particles_int)
         var_alloc(ff_particles_float)
         var_alloc(ff_ran_index)
         var_alloc(ff_ran_array)
         init=.false.
      else if (ff_number_particles > ff_particles_dim) then
         var_realloc(ff_particles_int,ff_particles_dim,ff_number_particles)
         var_realloc(ff_particles_float,ff_particles_dim,ff_number_particles)
         var_realloc(ff_ran_index,ff_particles_dim,ff_number_particles)
         var_realloc(ff_ran_array,ff_particles_dim,ff_number_particles)
         ff_particles_dim=ff_number_particles
      end if
@#if MASTER_SAMPLE
      call MPI_recv(ff_particles_int,array_size(ff_particles_int),mpi_int,
     $              mpi_degas2_root,tag,comm_world_dup,mpi_status,mpi_err)
      tag++
      call MPI_recv(ff_particles_float,array_size(ff_particles_float),mpi_real,
     $              mpi_degas2_root,tag,comm_world_dup,mpi_status,mpi_err)
      tag++
      call MPI_recv(ff_ran_index,array_size(ff_ran_index),mpi_int,
     $              mpi_degas2_root,tag,comm_world_dup,mpi_status,mpi_err)
      tag++
      call MPI_recv(ff_ran_array,array_size(ff_ran_array),mpi_real,
     $              mpi_degas2_root,tag,comm_world_dup,mpi_status,mpi_err)
@#else
      nflights=ff_number_particles
      call MPI_recv(nstart,1,mpi_int,mpi_degas2_root,tag,comm_world_dup,mpi_status,mpi_err)
      tag++
      call MPI_recv(isource,1,mpi_int,mpi_degas2_root,tag,comm_world_dup,mpi_status,mpi_err)
      tag++
      call MPI_recv(seed,ran_s,mpi_int,mpi_degas2_root,tag,comm_world_dup,mpi_status,mpi_err)
/*
   Sample the source particles and pack them into the 
   |flight_frag| object, conveyed here by |ff_common|.
*/
      call setup_flight_frag(nstart,nflights,so_args(isource),seed)
@#endif
@#if MPI_slave_messages
      write(diskout+3,*) ' End of receive calls; tag = ',tag
      call date_time(time)
      call date_string(time,date_str)
      write(diskout+3,*) 'Time = ',date_str
      call flush(diskout+3)
@#endif
      call do_flights(estimator_factors)
      done_flag=TRUE
      tag=200
@#if MPI_slave_messages
      write(diskout+3,*) ' End of receive calls; tag = ',tag
      call date_time(time)
      call date_string(time,date_str)
      write(diskout+3,*) 'Time = ',date_str
      call flush(diskout+3)
@#endif
      call MPI_send(done_flag,1,mpi_int,mpi_degas2_root,tag,comm_world_dup,mpi_err)
@#if MPI_slave_messages
      write(diskout+3,*) ' Sent done_flag; tag = ',tag
      call date_time(time)
      call date_string(time,date_str)
      write(diskout+3,*) 'Time = ',date_str
      call flush(diskout+3)
@#endif

      tag=300
@#if MPI_slave_messages
      write(diskout+3,*) ' Before receipt of send_data; tag = ',tag
      call date_time(time)
      call date_string(time,date_str)
      write(diskout+3,*) 'Time = ',date_str
      call flush(diskout+3)
@#endif
      call MPI_recv(send_data,1,mpi_int,mpi_degas2_root,tag,comm_world_dup,mpi_status,mpi_err)
@#if MPI_slave_messages
      write(diskout+3,*) ' After receipt of send_data; tag = ',tag
      call date_time(time)
      call date_string(time,date_str)
      write(diskout+3,*) 'Time = ',date_str
      call flush(diskout+3)
@#endif
      if (send_data == send_data_no) then
         goto loop
      else if ((send_data == send_data_meta_slave)
     $        || (send_data == send_data_all)) then
         min_level=1
         if (send_data == send_data_all) min_level=mpi_nlevels  // Skip aggregation
         do ilevel=min_level,mpi_nlevels
            if (mpi_slave_map_m(ilevel,mpi_rank) == mpi_rank) then
/*
   This is a meta-slave for this level.  Begin by merging
   its |stat_frag| data into |stat_fin|.
*/
               if (ilevel == min_level) then
                  @<Merge fin results@>
               end if
/*
   Now retreive data from all underlying slaves.  Note that we use
   |islave| here really just as a counter.  The order in which the
   slaves send data to this meta-slave is indeterminate, but also
   immaterial.  For diagnostic purposes, we can track it with the 
   returned |sending_slave|.  The slave data are deposited in
   |stat_frag| and then merged into |stat_fin|.  Note that |stat_frag|
   gets reset at the end of each call to |stat_acc| and is, thus,
   ready to be used again.  |stat_fin| is always being accumulated.
*/
               do islave=1,mpi_nslaves
                  if ((islave != mpi_rank) 
     $                 && (mpi_slave_map_m(ilevel,islave) == mpi_rank)) then
                     call slave_receive_flights(sending_slave)
                     @<Merge fin results@>
                  end if
               end do
            else
/*
   And {\em not} a meta-slave at this level  Only have to send
   data to the meta-slave for the next level.  At the last level, the
   destination is |mpi_degas2_root|.
*/
               if (ilevel == min_level) then
                  @<Send frag@>
               else
                  @<Send fin@>
               end if
/*
   The above ``send'' macros reset the sent |stat| arrays when they are 
   done so as to be ready for the next source group, etc.  These are
   the corresponding reinitializations for the snapshot paricle data.
   The same arrays are used whether this is a meta-slave or not.
*/
               sn_number_particles=0
               do i=1,sn_particles_dim
                  do j=1,sn_pt_float_max
                     sn_particles_float[i][j]=real_uninit
                  end do
                  do j=1,sn_pt_int_max
                     sn_particles_int[i][j]=int_uninit
                  end do
               end do
/*
   This slave is done with data aggregation.  It can go back to 
   the top of the routine (|loop|) to see if there are more
   flights via a new source group, etc.
*/
               goto loop
            end if
/*
   Make sure all meta-slaves are done with this level before 
   going to the next one.
*/
@#if MPI_slave_messages
      write(diskout+3,*) ' Before MPI_barrier'
      call flush(diskout+3)
@#endif
         call MPI_barrier(mpi_meta_slave_comms[ilevel],mpi_err)
@#if MPI_slave_messages
      write(diskout+3,*) ' mpi_err from MPI_barrier = ', mpi_err
      call flush(diskout+3)
@#endif


         end do       // Loop over aggregation levels
      else
         assert('Illegal value of send_data' == ' ')
      end if          // On |send_data|
/*
   Now everything is done.  Clean up and return.
*/
break:  continue
      call clear_stat
      var_free(estimator_factors)
      var_free(sn_particles_float)
      var_free(sn_particles_int)
/*
   Moved these to |bgk_flighttest|, outside BGK loop.
*/
@#if 0
      var_free(ff_particles_int)
      var_free(ff_particles_float)
      var_free(ff_ran_index)
      var_free(ff_ran_array)
@#endif
@#if MPI_slave_messages
      close(diskout+3)
@#endif

      return
      end
@#else
      subroutine do_flights_slave
// dummy
      return
      end
@#endif

@ Fill coupling arrays. This needs to be in a separate subroutine since
we don't set the dimensions until just before the call.

@m i_dens 1
@m i_temp 2
@m i_flux1 3
@m i_flux2 4
@m i_flux3 5
@m list_max 5  // However many of these there are 

@<Functions...@>=
      subroutine fill_coupling_arrays

      define_dimen(list_ind,list_max)

      define_varp(zone_map,INT,output_ind_1,output_ind_2)
      define_varp(test_data,FLOAT,output_ind_1,output_ind_2,problem_test_ind,list_ind)

      implicit_none_f77
      zn_common
      sp_common
      bk_common
      pr_common
      so_common
      tl_common
      ou_common                                                // Common
      implicit_none_f90
      
      integer zone,back,e_back,ix,iy,i,ix_1,ix_n,iy_1,iy_n,    // Local
     $        test,is,i_list,first_back
      integer index_parameters[tl_index_max]
      real multiplier
      logical have_pressure

      @<Memory allocation interface@>
      st_decls
      tl_decls
      zn_decls

      external extract_output_datum                            // External
      real extract_output_datum

      declare_varp(zone_map)
      declare_varp(test_data)

      var_alloc(zone_map)
      var_alloc(test_data)

      do zone=1,zn_num
         if ((zone_type[zone] == zn_plasma) 
     $        && (zone_index[zone][zi_ptr] == zone)
     $        && ((zone_index[zone][zi_ix] >= output_index_1_min)
     $           && (zone_index[zone][zi_ix] <= output_index_1_max)
     $           && (zone_index[zone][zi_iz] >= output_index_2_min)
     $           && (zone_index[zone][zi_iz] <= output_index_2_max))) then
            zone_map(zone_index[zone][zi_ix],zone_index[zone][zi_iz])=zone
         end if
      end do

/* 
   Note: with present configuration, these are volume-integrated.
   Also, this assumes that the tallies are all rank 2 (back and zone).
   If they are larger, the following could be readily generalized to
   sum over the extra dimensions.
*/

      ix_1=max(1,output_index_1_min)
      ix_n=output_index_1_max
      iy_1=max(1,output_index_2_min)
      iy_n=output_index_2_max

      if (string_lookup('neutral pressure',tally_name,tl_num) > 0) then
         have_pressure=.true.
      else
         have_pressure=.false.   // Not kept with minimal set of scores
      end if
      do test=2,pr_test_num    // Exclude ``0'' test species
         index_parameters[tl_index_test]=test
         do iy=iy_1,iy_n
            do ix=ix_1,ix_n
               if (zn_check(zone_map(ix,iy))) then
                  index_parameters[tl_index_zone]=zone_map(ix,iy)
                  test_data(ix,iy,test,i_dens)
     $                 =extract_output_datum(index_parameters,1,
     $                 out_post_all,o_mean,'neutral density')
                  if (have_pressure 
     $                 && test_data(ix,iy,test,i_dens) != zero) then
                     test_data(ix,iy,test,i_temp)
     $                    =extract_output_datum(index_parameters,1,
     $                    out_post_all,o_mean,'neutral pressure')
     $                    /test_data(ix,iy,test,i_dens)
                  else
                     test_data(ix,iy,test,i_temp)=zero
                  end if
                  do i=1,3
                     test_data(ix,iy,test,i_flux1+i-1)
     $                    =extract_output_datum(index_parameters,i,
     $                    out_post_all,o_mean,'neutral flux vector')
                  end do
               end if
            end do
         end do
      end do

      open(unit=diskout,file='testdata.out',status='unknown')
      write(diskout,*) ix_n-ix_1+1,iy_n-iy_1+1,pr_test_num-1
      do test=2,pr_test_num
         write(diskout,*) sp_sy(pr_test(test))
      end do
      multiplier=one
      do test=2,pr_test_num
         call write_sources(test_data(output_index_1_min,output_index_2_min,
     $        test,i_dens),multiplier,
     $        ix_1,ix_n,iy_1,iy_n,output_index_1_min,output_index_1_max,
     $        output_index_2_min,output_index_2_max,diskout)
         call write_sources(test_data(output_index_1_min,output_index_2_min,
     $        test,i_temp),multiplier,
     $        ix_1,ix_n,iy_1,iy_n,output_index_1_min,output_index_1_max,
     $        output_index_2_min,output_index_2_max,diskout)
      end do
      do test=2,pr_test_num
         do i_list=i_flux1,i_flux3
            call write_sources(test_data(output_index_1_min,output_index_2_min,
     $           test,i_list),multiplier,
     $           ix_1,ix_n,iy_1,iy_n,output_index_1_min,output_index_1_max,
     $           output_index_2_min,output_index_2_max,diskout)
         end do
      end do
      close(unit=diskout)
/*
   This output only makes sense in ``normal'' plasma simulations.
   The following test is probably not comprehensive, but should
   catch most exceptions.
*/         
      e_back=pr_background_lookup(sp_lookup('e'))
      if ((pr_reaction_num > 0) && (bk_check(e_back))) then  
         open(unit=diskout,file='sources.out',status='unknown')
         do is=1,so_grps
            first_back=0
            do back=1,pr_background_num
               index_parameters[tl_index_problem_sp]=pr_problem_sp_back(back)
               do iy=iy_1,iy_n
                  do ix=ix_1,ix_n
                     if (zn_check(zone_map(ix,iy))) then
                        index_parameters[tl_index_zone]=zone_map(ix,iy)
                        output_2D_coupling(ix,iy,back,ou_coupling_mass,is)=
     $                       extract_output_datum(index_parameters,1,
     $                       out_post_grp[is][0][o_mean],o_mean,
     $                       'ion source rate')
                        output_2D_coupling(ix,iy,back,
     $                       ou_coupling_momentum_1,is)=
     $                       extract_output_datum(index_parameters,1,
     $                       out_post_grp[is][0][o_mean],o_mean,
     $                       'ion momentum source vector')
                        output_2D_coupling(ix,iy,back,
     $                       ou_coupling_momentum_2,is)=
     $                       extract_output_datum(index_parameters,2,
     $                       out_post_grp[is][0][o_mean],o_mean,
     $                       'ion momentum source vector')
                        output_2D_coupling(ix,iy,back,
     $                       ou_coupling_momentum_3,is)=
     $                       extract_output_datum(index_parameters,3,
     $                       out_post_grp[is][0][o_mean],o_mean,
     $                       'ion momentum source vector')
/*
   Fluid codes presently track just one ion temperature. Sum all of the
   energy sources to non-electron background species into the
   |first_back| entry (others will be 0).
*/
                        if (first_back == 0 || back == e_back) then
                           output_2D_coupling(ix,iy,back,
     $                          ou_coupling_energy,is)=
     $                          extract_output_datum(index_parameters,1,
     $                          out_post_grp[is][0][o_mean],o_mean,
     $                          'ion energy source')
                        else
                           output_2D_coupling(ix,iy,first_back,
     $                          ou_coupling_energy,is)+=
     $                          extract_output_datum(index_parameters,1,
     $                          out_post_grp[is][0][o_mean],o_mean,
     $                          'ion energy source')
                        end if
                     else
                        do i=1,ou_coupling_max
                           output_2D_coupling(ix,iy,back,i,is)=zero
                        end do
                     end if
                  end do      // ix
               end do         // iy
               if (first_back == 0 && back != e_back) first_back = back
            end do            // back

            multiplier=one
            write(diskout,'(e12.6)') -so_scale(is)*so_tot_curr(is)
            do back=1,pr_background_num
               if (back != e_back) then
                  call write_sources(output_2D_coupling(output_index_1_min,
     $                 output_index_2_min,back,
     $                 ou_coupling_mass,is),multiplier,ix_1,ix_n,
     $                 iy_1,iy_n,output_index_1_min,output_index_1_max,
     $                 output_index_2_min,output_index_2_max,diskout)
                  call write_sources(output_2D_coupling(output_index_1_min,
     $                 output_index_2_min,back,
     $                 ou_coupling_momentum_1,is),multiplier,ix_1,ix_n,
     $                 iy_1,iy_n,output_index_1_min,output_index_1_max,
     $                 output_index_2_min,output_index_2_max,diskout)
                  call write_sources(output_2D_coupling(output_index_1_min,
     $                 output_index_2_min,back,
     $                 ou_coupling_momentum_2,is),multiplier,ix_1,ix_n,
     $                 iy_1,iy_n,output_index_1_min,output_index_1_max,
     $                 output_index_2_min,output_index_2_max,diskout)
                  call write_sources(output_2D_coupling(output_index_1_min,
     $                 output_index_2_min,back,
     $                 ou_coupling_momentum_3,is),multiplier,ix_1,ix_n,
     $                 iy_1,iy_n,output_index_1_min,output_index_1_max,
     $                 output_index_2_min,output_index_2_max,diskout)
               end if
            end do
            call write_sources(output_2D_coupling(output_index_1_min,
     $           output_index_2_min,e_back,
     $           ou_coupling_energy,is),multiplier,ix_1,ix_n,iy_1,iy_n,
     $           output_index_1_min,output_index_1_max,
     $           output_index_2_min,output_index_2_max,diskout)
            call write_sources(output_2D_coupling(output_index_1_min,
     $           output_index_2_min,first_back,
     $           ou_coupling_energy,is),multiplier,ix_1,ix_n,iy_1,iy_n,
     $           output_index_1_min,output_index_1_max,
     $           output_index_2_min,output_index_2_max,diskout)
         end do
         close(unit=diskout)
      end if

      var_free(zone_map)
      var_free(test_data)
     
      return
      end

@ Routine to write text data similar to EIRENE's \verb+fort.32+ and
\verb+fort.44+ files.

@<Functions...@>=
      subroutine write_sources(two_D_data,multiplier,ix_1,ix_n,
     $           iy_1,iy_n,dimx_1,dimx_n,dimy_1,dimy_n,unit_num)

      implicit_none_f77
      implicit_none_f90

      integer ix_1,ix_n,iy_1,iy_n,dimx_1,dimx_n,    // Input
     $        dimy_1,dimy_n,unit_num
      real two_D_data[dimx_1:dimx_n,dimy_1:dimy_n]
      real multiplier

      integer lim,iy,ix,j                           // Local

      lim=((ix_n-ix_1+1)/5)*5-4
      do iy=iy_1,iy_n
         do ix=1,lim,5
            write(unit_num,'(5(e16.8))') SP 
     $           (multiplier*two_D_data[ix_1+ix-1+j,iy],j=0,4)
         end do
         if (ix_1+lim+3 != ix_n) then
            write(unit_num,'(5(e16.8))') SP 
     $           (multiplier*two_D_data[ix,iy],ix=ix_1+lim+4,ix_n)
         end if
      end do

      return
      end
     
@ Write out data into netcdf file \verb+output.nc+ 

@ This is normally 0 (for ``32 bit offset'' files); set to 1 for large
files (``64 bit offset'').  This is only available for netCDF version 3.6
or later

@m NC_CLASSIC 0
@m NC_64_BIT 1
@m NC_HDF5 2  // AKA netcdf4
@m OUTPUT_NC_FILE_TYPE NC_64_BIT
@m SNAPSHOT_NC_FILE_TYPE NC_CLASSIC

@<Functions...@>=
      subroutine nc_write_output
      implicit_none_f77
      rf_common                  // Common
      so_common
      pr_common
      tl_common
      ou_common
      sn_common
      implicit_none_f90
      nc_decls
      st_decls
      tl_decls
      integer fileid             // Local
      ou_ncdecl
      sn_ncdecl
      character*LINELEN description,program_version
      character*FILELEN tempfile

      program_version=
     $     'DEGAS 2 Git commit: $Format:%H$, ref names: $Format:%d$'

      tempfile=filenames_array[outputfile]
      assert(tempfile != char_undef)

@#if (OUTPUT_NC_FILE_TYPE == NC_CLASSIC)
      fileid=nccreate(tempfile,NC_CLOBBER,nc_stat)
@#elseif (OUTPUT_NC_FILE_TYPE == NC_64_BIT)
      fileid=nccreate(tempfile,or(NC_CLOBBER,NF_64BIT_OFFSET),nc_stat)
@#else
      fileid=nccreate(tempfile,or(NC_CLOBBER,NF_NETCDF4),nc_stat)
@#endif

      description = 'Output from DEGAS 2'

      call ncattputc(fileid,NC_GLOBAL,'description',NC_CHAR,
     $     string_length(description),
     $     description,nc_stat)

      call ncattputc(fileid,NC_GLOBAL,'program_version',NC_CHAR,
     $     string_length(program_version),
     $     program_version,nc_stat)

      ou_ncdef(fileid)
      call ncendef(fileid,nc_stat)
      ou_ncwrite(fileid)

      call ncclose(fileid,nc_stat)

      if (so_time_dependent == TRUE) then
         tempfile=filenames_array[snapshotfile]
         assert(tempfile != char_undef)
@#if (SNAPSHOT_NC_FILE_TYPE == NC_CLASSIC)
         fileid=nccreate(tempfile,NC_CLOBBER,nc_stat)
@#elseif (SNAPSHOT_NC_FILE_TYPE == NC_64_BIT)
      fileid=nccreate(tempfile,or(NC_CLOBBER,NF_64BIT_OFFSET),nc_stat)
@#else
      fileid=nccreate(tempfile,or(NC_CLOBBER,NF_NETCDF4),nc_stat)
@#endif
         sn_ncdef(fileid)
         call ncendef(fileid,nc_stat)
         sn_ncwrite(fileid)
         call ncclose(fileid,nc_stat)
      end if

      return
      end

@ Clear output arrays.

@<Functions...@>=
      subroutine clear_output
      implicit_none_f77
      so_common
      pr_common
      tl_common
      ou_common
      sn_common
      implicit_none_f90
      tl_decls

      @<Memory allocation interface@>

      var_free(output_all)
      var_free(output_grp)
      var_free(output_weight_grp)
      var_free(output_num_flights)
      var_free(output_random_seed)
      var_free(out_post_all)
      var_free(out_post_grp)
      var_free(output_2D_coupling)

      var_free(sn_particles_float)
      var_free(sn_particles_int)
/*
   These have been moved to |bgk_flighttest|, outside
   the BGK loop.
*/
@#if 0
      var_free(ff_particles_int)
      var_free(ff_particles_float)
      var_free(ff_ran_index)
      var_free(ff_ran_array)
@#endif

      return
      end

@* INDEX.
